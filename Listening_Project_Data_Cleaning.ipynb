{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRXDbn9hbVjH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Tuple\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "# Upload the dataset interactively in Colab\n",
        "uploaded = files.upload()\n",
        "input_path = list(uploaded.keys())[0]\n",
        "cleaned_output_path = \"nightline_cleaned_final.jsonl\"\n",
        "instruction_output_path = \"nightline_instruction_dataset.jsonl\"\n",
        "dpo_output_path = \"nightline_dpo_dataset.jsonl\"\n",
        "\n",
        "# Patterns to strip from the start of responses (exclude directive-intended phrases)\n",
        "STRIP_PATTERNS = [\n",
        "    r\"^(directive|neutral|nightline-aligned)[:\\s]+\",\n",
        "    r\"^response[:\\s]+\",\n",
        "    r\"^option \\d+[:\\s]+\",\n",
        "    r\"^\\d+\\.\\s+\",\n",
        "    r\"^here is your response[:\\s]+\",\n",
        "    r\"^as an ai language model[:\\s]+\",\n",
        "    r\"^i am an ai[:\\s]+\"\n",
        "]\n",
        "\n",
        "def load_jsonl(file_path: str):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return [json.loads(line.strip()) for line in f if line.strip()]\n",
        "\n",
        "def save_jsonl(data: list, output_path: str):\n",
        "    with open(output_path, 'w') as f:\n",
        "        for item in data:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "def normalize_traits(traits: Dict[str, Any]) -> Dict[str, bool]:\n",
        "    expected_traits = ['non_directive', 'judgment_free', 'empathic_reflection', 'escalation_safe', 'empathy_score']\n",
        "    return {key: bool(traits.get(key, False)) if key != 'empathy_score' else int(traits.get(key, 0)) for key in expected_traits}\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    lowered = text.lower().strip()\n",
        "    for pattern in STRIP_PATTERNS:\n",
        "        if re.match(pattern, lowered):\n",
        "            return re.sub(pattern, '', text, flags=re.IGNORECASE).strip()\n",
        "    return text.strip()\n",
        "\n",
        "def clean_entry(entry: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    cleaned = {\n",
        "        \"prompt\": entry.get(\"prompt\", \"\").strip(),\n",
        "        \"responses\": {}\n",
        "    }\n",
        "\n",
        "    for key in ['directive', 'neutral', 'nightline_aligned']:\n",
        "        response = entry.get(\"responses\", {}).get(key, {})\n",
        "        text = clean_text(response.get(\"text\", \"\"))\n",
        "\n",
        "        cleaned_response = {\n",
        "            \"text\": text,\n",
        "            \"traits\": normalize_traits(response.get(\"traits\", {}))\n",
        "        }\n",
        "        cleaned[\"responses\"][key] = cleaned_response\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "def extract_instruction_data(cleaned_data: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
        "    instruction_pairs = []\n",
        "    for entry in cleaned_data:\n",
        "        prompt = entry.get(\"prompt\", \"\").strip()\n",
        "        aligned = entry.get(\"responses\", {}).get(\"nightline_aligned\", {}).get(\"text\", \"\").strip()\n",
        "        if prompt and aligned:\n",
        "            instruction_pairs.append({\"prompt\": prompt, \"response\": aligned})\n",
        "    return instruction_pairs\n",
        "\n",
        "def extract_dpo_data(cleaned_data: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
        "    dpo_pairs = []\n",
        "    for entry in cleaned_data:\n",
        "        prompt = entry.get(\"prompt\", \"\").strip()\n",
        "        aligned = entry.get(\"responses\", {}).get(\"nightline_aligned\", {}).get(\"text\", \"\").strip()\n",
        "        directive = entry.get(\"responses\", {}).get(\"directive\", {}).get(\"text\", \"\").strip()\n",
        "        neutral = entry.get(\"responses\", {}).get(\"neutral\", {}).get(\"text\", \"\").strip()\n",
        "\n",
        "        if prompt and aligned and directive:\n",
        "            dpo_pairs.append({\"prompt\": prompt, \"chosen\": aligned, \"rejected\": directive})\n",
        "        if prompt and aligned and neutral:\n",
        "            dpo_pairs.append({\"prompt\": prompt, \"chosen\": aligned, \"rejected\": neutral})\n",
        "    return dpo_pairs\n",
        "\n",
        "def process_dataset(input_path: str, cleaned_output_path: str, instruction_output_path: str, dpo_output_path: str):\n",
        "    data = load_jsonl(input_path)\n",
        "    cleaned_data = [clean_entry(entry) for entry in data if entry.get(\"prompt\") and entry.get(\"responses\")]\n",
        "    save_jsonl(cleaned_data, cleaned_output_path)\n",
        "    print(f\"Cleaned entries: {len(cleaned_data)}\")\n",
        "\n",
        "    instruction_data = extract_instruction_data(cleaned_data)\n",
        "    save_jsonl(instruction_data, instruction_output_path)\n",
        "    print(f\"Instruction-tuning entries: {len(instruction_data)}\")\n",
        "\n",
        "    dpo_data = extract_dpo_data(cleaned_data)\n",
        "    save_jsonl(dpo_data, dpo_output_path)\n",
        "    print(f\"DPO pairs generated: {len(dpo_data)}\")\n",
        "\n",
        "    files.download(cleaned_output_path)\n",
        "    files.download(instruction_output_path)\n",
        "    files.download(dpo_output_path)\n",
        "\n",
        "# Run immediately in Colab\n",
        "process_dataset(input_path, cleaned_output_path, instruction_output_path, dpo_output_path)\n"
      ]
    }
  ]
}