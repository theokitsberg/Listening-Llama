{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAo1Yc1h5Ztw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def clean_text(text):\n",
        "    return text.replace(\"\\\\'\", \"'\").replace('\\\\\"', '\"').replace(\"\\\\n\", \"\\n\").strip()\n",
        "\n",
        "def clean_all_text_fields(entry):\n",
        "    entry[\"prompt\"] = clean_text(entry.get(\"prompt\", \"\"))\n",
        "    for key in [\"directive\", \"neutral\", \"nightline_aligned\"]:\n",
        "        if key in entry.get(\"responses\", {}):\n",
        "            response_text = entry[\"responses\"][key].get(\"text\", \"\")\n",
        "            entry[\"responses\"][key][\"text\"] = clean_text(response_text)\n",
        "    return entry\n",
        "\n",
        "# Load raw cleaned dataset (upload manually or use path if already in Colab)\n",
        "input_file = \"nightline_cleaned_final(1).jsonl\"\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_entries = [json.loads(line.strip()) for line in f]\n",
        "\n",
        "# Clean the dataset\n",
        "cleaned_full_dataset = [clean_all_text_fields(entry) for entry in raw_entries]\n",
        "\n",
        "# Save CLEANED FINAL TOTAL NIGHTLINE DATA\n",
        "with open(\"CLEANED_FINAL_TOTAL_NIGHTLINE_DATA.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in cleaned_full_dataset:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# Extract and save CLEANED FINAL INSTRUCTION DATA\n",
        "instruction_data = [\n",
        "    {\n",
        "        \"prompt\": entry[\"prompt\"],\n",
        "        \"response\": entry[\"responses\"][\"nightline_aligned\"][\"text\"]\n",
        "    }\n",
        "    for entry in cleaned_full_dataset\n",
        "    if \"nightline_aligned\" in entry[\"responses\"]\n",
        "]\n",
        "\n",
        "with open(\"CLEANED_FINAL_INSTRUCTION_DATA.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in instruction_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# Extract and save CLEANED FINAL DPO DATA\n",
        "dpo_data = []\n",
        "for entry in cleaned_full_dataset:\n",
        "    prompt = entry[\"prompt\"]\n",
        "    aligned = entry[\"responses\"].get(\"nightline_aligned\", {}).get(\"text\", \"\")\n",
        "    directive = entry[\"responses\"].get(\"directive\", {}).get(\"text\", \"\")\n",
        "    neutral = entry[\"responses\"].get(\"neutral\", {}).get(\"text\", \"\")\n",
        "\n",
        "    if prompt and aligned and directive:\n",
        "        dpo_data.append({\"prompt\": prompt, \"chosen\": aligned, \"rejected\": directive})\n",
        "    if prompt and aligned and neutral:\n",
        "        dpo_data.append({\"prompt\": prompt, \"chosen\": aligned, \"rejected\": neutral})\n",
        "\n",
        "with open(\"CLEANED_FINAL_DPO_DATA.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in dpo_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"✅ Done cleaning and exporting:\")\n",
        "print(f\"• CLEANED_FINAL_TOTAL_NIGHTLINE_DATA.jsonl → {len(cleaned_full_dataset)} entries\")\n",
        "print(f\"• CLEANED_FINAL_INSTRUCTION_DATA.jsonl → {len(instruction_data)} entries\")\n",
        "print(f\"• CLEANED_FINAL_DPO_DATA.jsonl → {len(dpo_data)} pairs\")\n"
      ]
    }
  ]
}